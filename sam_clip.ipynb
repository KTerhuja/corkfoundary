{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q 'git+https://github.com/facebookresearch/segment-anything.git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p {HOME}/weights\n",
    "! wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -P {HOME}/weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import clip\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SAM_CHECKPOINT = \"/content/weights/sam_vit_h_4b8939.pth\"  # Download from https://github.com/facebookresearch/segment-anything\n",
    "MODEL_TYPE = \"vit_h\"\n",
    "CLIP_MODEL_NAME = \"ViT-B/32\"  # Faster than ViT-L/14\n",
    "SIMILARITY_THRESHOLD = 0.9    # Adjust based on your use case\n",
    "\n",
    "# Initialize models\n",
    "def initialize_models():\n",
    "    # Load SAM\n",
    "    sam = sam_model_registry[MODEL_TYPE](checkpoint=SAM_CHECKPOINT)\n",
    "    sam.to(device=DEVICE)\n",
    "    mask_generator = SamAutomaticMaskGenerator(\n",
    "        sam,\n",
    "        points_per_side=32,  # Reduce for faster processing\n",
    "        pred_iou_thresh=0.86,\n",
    "        stability_score_thresh=0.92,\n",
    "        crop_n_layers=1\n",
    "    )\n",
    "    \n",
    "    # Load CLIP\n",
    "    clip_model, clip_preprocess = clip.load(CLIP_MODEL_NAME, device=DEVICE)\n",
    "    \n",
    "    return mask_generator, clip_model, clip_preprocess\n",
    "\n",
    "# Process reference image\n",
    "def get_reference_embedding(image_path, clip_model, clip_preprocess):\n",
    "    ref_image = Image.open(image_path).convert(\"RGB\")\n",
    "    preprocessed = clip_preprocess(ref_image).unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        embedding = clip_model.encode_image(preprocessed)\n",
    "    return embedding / embedding.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Process target image and detect objects\n",
    "def detect_objects(target_path, mask_generator, clip_model, clip_preprocess, ref_embedding):\n",
    "    # Load target image\n",
    "    target_image = Image.open(target_path).convert(\"RGB\")\n",
    "    target_np = np.array(target_image)\n",
    "    \n",
    "    # Generate masks with SAM\n",
    "    masks = mask_generator.generate(target_np)\n",
    "    \n",
    "    # Process each candidate region\n",
    "    detected_regions = []\n",
    "    for mask_info in masks:\n",
    "        mask = mask_info[\"segmentation\"]\n",
    "        y, x = np.where(mask)\n",
    "        if len(x) == 0 or len(y) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Get bounding box\n",
    "        x_min, x_max = np.min(x), np.max(x)\n",
    "        y_min, y_max = np.min(y), np.max(y)\n",
    "        \n",
    "        # Crop region (expand slightly for context)\n",
    "        padding = 3\n",
    "        crop = target_image.crop((\n",
    "            max(0, x_min - padding),\n",
    "            max(0, y_min - padding),\n",
    "            min(target_image.width, x_max + padding),\n",
    "            min(target_image.height, y_max + padding)\n",
    "        ))\n",
    "        \n",
    "        # Get CLIP embedding for the region\n",
    "        preprocessed = clip_preprocess(crop).unsqueeze(0).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            region_embedding = clip_model.encode_image(preprocessed)\n",
    "        region_embedding /= region_embedding.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Calculate similarity\n",
    "        similarity = torch.matmul(ref_embedding, region_embedding.T).item()\n",
    "        \n",
    "        if similarity > SIMILARITY_THRESHOLD:\n",
    "            detected_regions.append({\n",
    "                \"bbox\": (x_min, y_min, x_max, y_max),\n",
    "                \"similarity\": similarity,\n",
    "                \"mask\": mask\n",
    "            })\n",
    "    \n",
    "    return target_image, detected_regions\n",
    "\n",
    "# Visualization\n",
    "def visualize_results(image, regions):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for region in sorted(regions, key=lambda x: x[\"similarity\"], reverse=True):\n",
    "        x_min, y_min, x_max, y_max = region[\"bbox\"]\n",
    "        draw.rectangle([x_min, y_min, x_max, y_max], outline=\"red\", width=2)\n",
    "        draw.text((x_min, y_min), \n",
    "                 f\"{region['similarity']:.2f}\", \n",
    "                 fill=\"white\")\n",
    "    return image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize models\n",
    "    sam_mask_generator, clip_model, clip_preprocess = initialize_models()\n",
    "    # Get reference embedding\n",
    "    ref_embedding = get_reference_embedding(\n",
    "        \"/content/sample1.png\", \n",
    "        clip_model, \n",
    "        clip_preprocess\n",
    "    )\n",
    "\n",
    "    # Detect objects in target image\n",
    "    target_image, regions = detect_objects(\n",
    "        \"/content/reference_img.png\",\n",
    "        sam_mask_generator,\n",
    "        clip_model,\n",
    "        clip_preprocess,\n",
    "        ref_embedding\n",
    "    )\n",
    "\n",
    "    # Visualize and save\n",
    "    result_image = visualize_results(target_image, regions)\n",
    "    result_image.save(\"detection_results.jpg\")\n",
    "    print(f\"Found {len(regions)} matching regions\")\n",
    "    \n",
    "   "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
